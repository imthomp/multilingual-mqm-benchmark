For my semester project, I'm going to use human error annotations collected by my lab to evaluate how well automated MT metrics actually agree with human judgment. Native speakers of twelve languages annotated thousands of translation segments using a simplified MQM taxonomy with error categories like mistranslation, omission, grammar, and register. The languages span a wide resource range: high-resource (Spanish, Portuguese), medium-resource (Thai, Croatian, Tagalog, Armenian), and low-resource (Haitian Creole, Lao, Marshallese, Navajo, Kiribati, Tongan). I chose this project because the standard benchmarks used to evaluate MT metrics only cover a few high-resource languages, and I have access to rare human annotation data that could fill that gap. My plan is to convert the span-level annotations to sentence-level scores and compute correlation against BLEU, ChrF++, BERTScore, COMET, xCOMET, and GEMBA-MQM across all twelve languages. For analysis I want to see how metric performance degrades as languages get lower-resource, whether different metrics handle accuracy vs. fluency errors differently, and whether metrics catch non-translations. I also plan to contribute the benchmark as a reusable module to Dr. Fulda's evaluation harness.